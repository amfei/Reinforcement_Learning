{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP7bbiMCD6FuifwHTGx5u0h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amfei/Reinforcement_Learning/blob/main/Q_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Mountain Car environment simulates a simple physics problem where an underpowered car must drive up a steep hill. The car’s engine is not strong enough to climb the hill in a direct route, so the car must learn to leverage potential energy by driving back and forth to build up enough momentum.\n",
        "\n",
        "###The environment's dynamics are defined by the equations of motion for the car's position and velocity."
      ],
      "metadata": {
        "id": "kbXppKu6yPH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MountainCarEnv:\n",
        "\n",
        "    '''\n",
        "    The environment simulates a one-dimensional mountain car where the goal is\n",
        "    to drive the car up a steep mountain.\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        self.min_position = -1.2\n",
        "        self.max_position = 0.6\n",
        "        self.goal_position = 0.5\n",
        "        self.max_speed = 0.07\n",
        "        #power: The amount of force applied when accelerating.\n",
        "        #gravity: The effect of gravity on the car, modeled by a cosine function.\n",
        "        self.power = 0.0015\n",
        "        self.gravity = -0.0025\n",
        "        self.low = np.array([self.min_position, -self.max_speed])\n",
        "        self.high = np.array([self.max_position, self.max_speed])\n",
        "\n",
        "    def reset(self):\n",
        "        '''reset method initializes the car's position and velocity. The position is randomly set between -0.6 and -0.4, and the velocity is set to 0.'''\n",
        "\n",
        "        self.position = np.random.uniform(low=-0.6, high=-0.4)\n",
        "        self.velocity = 0\n",
        "        return np.array([self.position, self.velocity])\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        '''The step method updates the car's position and velocity based on the action taken.\n",
        "\n",
        "        Take an action in the environment.\n",
        "\n",
        "        Returns : tuple: (next_state, reward, done, info)\n",
        "\n",
        "        '''\n",
        "\n",
        "        position, velocity = self.position, self.velocity\n",
        "        #The term (action−1)×power applies the force to accelerate left (-1), do nothing (0), or accelerate right (+1).\n",
        "        #The term cos(3×position)×gravity models the effect of gravity on the car based on its position.\n",
        "        velocity += (action - 1) * self.power + math.cos(3 * position) * self.gravity\n",
        "        velocity = np.clip(velocity, -self.max_speed, self.max_speed)\n",
        "        #The car's position is updated by adding the velocity to the current position\n",
        "        position += velocity\n",
        "        #Both position and velocity are clipped to ensure they stay within the defined limits\n",
        "        position = np.clip(position, self.min_position, self.max_position)\n",
        "        if position == self.min_position and velocity < 0:\n",
        "            velocity = 0\n",
        "\n",
        "\n",
        "        #The episode ends when the car's position reaches or exceeds the goal position\n",
        "        done = bool(position >= self.goal_position)\n",
        "        #The reward is -1 for each time step until the car reaches the goal, at which point the reward is 0\n",
        "        reward = -1.0 if not done else 0.0  #\n",
        "\n",
        "        self.position, self.velocity = position, velocity\n",
        "        return np.array([self.position, self.velocity]), reward, done, {}\n",
        "\n",
        "    def render(self):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(np.linspace(self.min_position, self.max_position, 100), np.sin(3 * np.linspace(self.min_position, self.max_position, 100)))\n",
        "        plt.plot(self.position, np.sin(3 * self.position), 'o', markersize=12, label='Car')\n",
        "        plt.axhline(y=np.sin(3 * self.goal_position), color='r', linestyle='--', label='Goal')\n",
        "        plt.title('Mountain Car')\n",
        "        plt.xlabel('Position')\n",
        "        plt.ylabel('Height')\n",
        "        plt.legend()\n",
        "        plt.grid()\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "gS7sxwoAyRoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning implementation\n",
        "q_table_size = [20, 20]\n",
        "action_space = 3  # Actions: 0 (left), 1 (no action), 2 (right)\n",
        "q_table = np.random.uniform(low=-1, high=1, size=(q_table_size + [action_space]))\n",
        "q_table.shape"
      ],
      "metadata": {
        "id": "4tTywsEnzeeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def discretize_state(state, env, q_table_size):\n",
        "    '''To simplify the continuous values of location and velocity into discrete categories,\n",
        "    I’ll convert each into 20 distinct bins.\n",
        "    '''\n",
        "\n",
        "    env_low = env.low\n",
        "    env_high = env.high\n",
        "    env_dx = (env_high - env_low) / q_table_size\n",
        "    discrete_state = (state - env_low) / env_dx\n",
        "    return tuple(discrete_state.astype(int))\n",
        "\n",
        "def calculate_q_value(current_state_qvalue, reward, next_state_qvalue, learning_rate=0.1, discount_factor=0.9):\n",
        "    return (1 - learning_rate) * current_state_qvalue + learning_rate * (reward + discount_factor * next_state_qvalue)\n",
        "\n",
        "def select_action(q_table, state, epsilon=0):\n",
        "    if np.random.random() >= epsilon:\n",
        "        # Take an random action by using q-value\n",
        "        action = np.argmax(q_table[state])\n",
        "    else:\n",
        "        # Take an action by random\n",
        "        action = np.random.choice(range(action_space))\n",
        "    return action\n",
        "\n",
        "def update_q_table(q_table, state, action, next_state, reward, learning_rate=0.1, discount_factor=0.9):\n",
        "    current_q = q_table[state][action]\n",
        "    max_future_q = np.max(q_table[next_state])\n",
        "    new_q = calculate_q_value(current_q, reward, max_future_q, learning_rate, discount_factor)\n",
        "    q_table[state][action] = new_q\n",
        "    return q_table\n",
        "\n",
        "env = MountainCarEnv()\n",
        "episodes = 500\n",
        "epsilon = 1.0\n",
        "epsilon_decay = 0.9995\n",
        "min_epsilon = 0.01\n",
        "\n",
        "for e in range(episodes):\n",
        "    current_state_raw = env.reset()\n",
        "    current_state = discretize_state(current_state_raw, env, q_table_size)\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        action = select_action(q_table, current_state, epsilon)\n",
        "        next_state_raw, reward, done, _ = env.step(action)\n",
        "        next_state = discretize_state(next_state_raw, env, q_table_size)\n",
        "        q_table = update_q_table(q_table, current_state, action, next_state, reward)\n",
        "        current_state = next_state\n",
        "\n",
        "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
        "\n",
        "# Test the agent\n",
        "state_raw = env.reset()\n",
        "state = discretize_state(state_raw, env, q_table_size)\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action = select_action(q_table, state, epsilon=0)\n",
        "    next_state_raw, reward, done, _ = env.step(action)\n",
        "    next_state = discretize_state(next_state_raw, env, q_table_size)\n",
        "    env.render()  # Implement this method if you want to visualize the environment\n",
        "    state = next_state\n"
      ],
      "metadata": {
        "id": "aO_r4wBKd6SO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}